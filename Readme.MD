# Notes
CoRL Day 1
Interesting talks/Food for thought. 

## Keynote speaker: Sangbae Kim

- mimicing biology is hard and bad. Should stay away.
- MIT actuator sold on Alibaba. Developed at MIT. Used in all modern robotics companies such as Unitree, etc. 
- Reference follow up was the actuator design and modeling: 
1. Proprioceptive Actuator Design in the MIT Cheetah
2. Design of a High Torque Density Modular Actuator for Dynamic Robots by Alexander Hattori
3. A Low Cost Modular Actuator for Dynamic Robots by Benjamin G. Katz


## Talk by Samsung Head of Robotics (limit sharing since corporate interest conflict)

- Flashback of early stage robotics competitions such as DARPA challenge. COntrolled using specific joint controls instead of modern day reinforcement learning. Won but a lot of the motion was teleoperated. Other sensors used were computer vision. 
-  Growing uncertainity of the future of humanoids, direction focus, application areas, Uniformity. Called for some kind of standardisation
- Noted observation, compared to past, the advent of LLM have brought about a new renewed focus on humanoids as it also learns faster and performs functions better. 
- Need some sort of generalized world models 


# Reviewed Papers

## Sound of simulation 
- Using sound as a modality the robots are able to better learn tasks, such a pouring water in a dark room. Vision language action sound model. Used clip with spectrogram. 

Link: https://multigen-audio.github.io/  CODE available as well

## DexSkin: High-Coverage Conformable Robotic Skin for Learning Contact-Rich Manipulation

https://www.arxiv.org/abs/2509.18830
-no code

- Grasping objects by robots is now primarily done through without using any force feedback. Without this, the fingers are unable to determine how much squeeezing should happen. 
- Overtime wear, difference between different robots hardware, prevent reusing the same force information. 
- If squeezed much, the object could potentially crack or break. 
- Development of a tactile feedback strip which is based on the measure of capacitance- gap between the electrode plates. 


## Research attempt and trial
Attempted to create a pipeline which taken in youtube videos of FPV say cooking, instrument playing etc. Uses hand pose estimation NN --> extracts the 5 and multi bone digit hand finger estimates --> applies or retargets to a hand URDF on simulation. 

## Bottlenecks:
- Hand First Person view videos are limited in number. 
- Retageting is not accurate or the approximation is not great. Human hand is 5 fingers whereas the hand of several of the devices used are in 3 or 4 fingers. 
### Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations
- Reinforcement learning has been used in the past to train objects to perform certain tasks. Just using RL has shown the grasping etc is unlike typical of human movement or motion. Demonstration using human motion capture is used as teacher/priviledged information. 

## Code run
# GeoRT
https://github.com/skr3178/GeoRT

- Instead of allegro hand, a different kind of hand was used for retargeting purposes. Hand was Unitree 5 finger and 3 finger hands. 
Facing issues of segmentation. 

- GeoRT: uses a finger to finger retargeting as well as the wrist tracking for effectiveness. 
- Key is to map the entire C-shape and the motion of the hand in proper direction.


CoRL Day 2

Presentation: ImMimic

Hand data --> Youtube videos -->skeleton or extraction of the hand pose
Retargeted to a particular hand
Motion is transferred. However, there are some limitations, the motion transfer process is not very accurate. Linear interpolation is used to retarget the data mismatch. say between 5 fingers captured to 3 fingers. 

### To prevent occulusion in hand tracking
MVHM: A Large-Scale Multi-View Hand Mesh Benchmark for Accurate 3D Hand Pose Estimation

- used a larger dataset along with more than a single camera to track the motion of hand
- available Mano Dataset for hands

### ViViDex: Learning Vision-based Dexterous Manipulation from Human Videos
- Available tool for extraction of hand only from the entire video: Reference trajectory extraction
- Code for hand motion extraction is available 

## Open source VR Hand projects:
- $250 : https://www.eozvr.com/products/eoz-immersive-vr-gloves?srsltid=AfmBOoriUfFDi_kDJgRAkE9l_D-nbFwH4wtbolGr-Dxmjl3Zru7tkuHa

